{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nouman-aziz/demo_bot/venv/lib/python3.12/site-packages/langchain_community/llms/openai.py:255: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/nouman-aziz/demo_bot/venv/lib/python3.12/site-packages/langchain_community/llms/openai.py:1089: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(model_name=\"gpt-4o-mini\")  # or any other model of your choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# Specify the path to your .docx file\n",
    "file_path = \"/home/nouman-aziz/Downloads/Vetting_Questions.docx\"\n",
    "\n",
    "# Initialize the loader\n",
    "loader = Docx2txtLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nouman-aziz/demo_bot/venv/lib/python3.12/site-packages/langsmith/client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        **Objective:**\n",
    "        As an AI language model, your task is to generate a sequence of five relevant questions that build upon the provided conversation history. The goal is to ensure that each question logically follows the previous one, maintaining coherence and continuity in the dialogue.\n",
    "\n",
    "        **Instructions:**\n",
    "        1. **Review the Conversation History:**\n",
    "        - Analyze the provided chat history to understand the context and flow of the conversation.\n",
    "        - Assess whether the user's previous responses adequately address the preceding questions.\n",
    "\n",
    "        2. **Generate Relevant Questions:**\n",
    "        - If the user's response to a question is satisfactory, proceed by formulating the next question in the sequence.\n",
    "        - If the user's response is incomplete or unsatisfactory, rephrase and repeat the previous question to elicit a more comprehensive answer.\n",
    "\n",
    "        3. **Sequence and Coherence:**\n",
    "        - Ensure that the sequence consists of five questions that are contextually relevant and logically connected.\n",
    "        - Each question should naturally follow from the previous one, fostering a coherent and engaging dialogue.\n",
    "\n",
    "        **Conversation History:**\n",
    "        {chat_history}\n",
    "\n",
    "        **Sample Questions:**\n",
    "        To guide the formulation of your questions, consider the following examples:\n",
    "        - What motivates your current job or service search?\n",
    "        - Would you describe your current work or mission as a calling?\n",
    "        - How would you articulate your life mission in a few words?\n",
    "        - How significant is your faith in influencing your career choices?\n",
    "        - Do you prefer working with organizations that share your faith or values?\n",
    "        - What is your current city and country of residence?\n",
    "        - What is your nationality?\n",
    "        - Are you open to relocating? If yes, where?\n",
    "        - What types of work are you interested in?\n",
    "        - What work environment do you prefer?\n",
    "\n",
    "        **Output Format:**\n",
    "        - Present the generated questions in a numbered list format.\n",
    "        - Ensure clarity and conciseness in each question.\n",
    "\n",
    "        **Example Output:**\n",
    "        1. What motivates your current job or service search?\n",
    "        2. How would you describe your life mission in a few words?\n",
    "        3. How significant is your faith in influencing your career choices?\n",
    "        4. Are you open to relocating? If yes, where?\n",
    "        5. What types of work are you interested in?\n",
    "\n",
    "        By adhering to these guidelines, you will create a structured and coherent sequence of questions that effectively continues the dialogue based on the provided conversation history.\n",
    "\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "                Objective:\n",
    "\n",
    "                Your task is to analyze the provided chat history and the latest user question, which may reference earlier parts of the conversation. Your goal is to ensure that the question can stand alone and be understood without requiring prior context.\n",
    "\n",
    "                Instructions:\n",
    "\n",
    "                Review the Conversation History:\n",
    "\n",
    "                Analyze the flow of the conversation and how the latest user query relates to previous messages.\n",
    "\n",
    "                Identify whether the user's latest question depends on prior context.\n",
    "\n",
    "                Reformulate the Question (if needed):\n",
    "\n",
    "                If the question references earlier parts of the conversation, restate it in a way that includes the missing context.\n",
    "\n",
    "                If the question is already self-contained, return it as is.\n",
    "\n",
    "                Ensure Clarity and Coherence:\n",
    "\n",
    "                The reformulated question should be concise and fully understandable on its own.\n",
    "\n",
    "                Preserve the original intent of the user's query while making necessary adjustments.\n",
    "\n",
    "                Output Format:\n",
    "\n",
    "                Provide the reformulated question in a clear and direct manner.\n",
    "\n",
    "                Do not provide an answerâ€”only restate the question in a standalone format.\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"Objective:\n",
    "You are an AI assistant responsible for answering user queries using retrieved context. Your goal is to provide accurate and concise responses while ensuring relevance to the question.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Utilize Retrieved Context:\n",
    "\n",
    "Use the provided context to generate an informed response.\n",
    "\n",
    "If the answer is explicitly stated in the context, summarize it succinctly.\n",
    "\n",
    "Handle Unanswered Questions:\n",
    "\n",
    "If the context does not contain sufficient information, clearly state that you do not know the answer.\n",
    "\n",
    "Do not fabricate information or make assumptions beyond the provided context.\n",
    "\n",
    "Maintain Clarity and Conciseness:\n",
    "\n",
    "Limit your response to a maximum of three sentences.\n",
    "\n",
    "Ensure that the response directly addresses the question without unnecessary elaboration.\n",
    "\n",
    "\n",
    "Output Format:\n",
    "\n",
    "Provide a direct and informative answer based on the retrieved context.\n",
    "\n",
    "If user response is not clear, respond with the previous question asked again until he gives better answer or skips the question.\n",
    "\n",
    "Retrieved Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
